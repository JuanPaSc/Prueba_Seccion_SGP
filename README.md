# Prueba_Seccion_SGP
Repositorio creado con la Ãºnica finalidad de desarrollar la prueba tÃ©cnica de ciencia de datos para la vacante de AnalÃ­tico (Junior) SecciÃ³n GestiÃ³n Proveedores

# README

## Proyecto AnalÃ­tico: Dataset de Transacciones Financieras

Este proyecto se centra en el anÃ¡lisis de un conjunto de datos de registros de transacciones, informaciÃ³n del consumidor y datos de cartas de una instituciÃ³n bancaria durante la dÃ©cada de los 2010s.

---

## Estructura del Proyecto

El proyecto estÃ¡ dividido en los siguientes mÃ³dulos con el propÃ³sito de resolver las diferentes actividades:

1. **Carga y Limpieza de Datos (`cargar_data.py`)**
   - Clase `cargar_data`: Permite cargar datos desde archivos CSV, limpiar valores nulos y duplicados, y detectar valores atÃ­picos (outliers) en columnas especÃ­ficas.

2. **AnÃ¡lisis Exploratorio (`analisis_exp.py`)**
   - Clase `ExploratoryAnalysis`: Incluye mÃ©todos para calcular promedios de valoraciones, contar reseÃ±as y valoraciones totales, y determinar los autores y categorÃ­as mÃ¡s populares.

3. **AnÃ¡lisis de Sentimientos (`analisis_NLP.py`)**
   - Clase `SentimentAnalysis`: Proporciona herramientas para preprocesar textos de reseÃ±as, calcular puntajes de sentimientos usando TextBlob y agrupar el promedio de sentimientos por libros o categorÃ­as.

4. **AnÃ¡lisis de Libros Principales (`top_libros.py`)**
   - Clase `TopBooksAnalysis`: Identifica los libros mÃ¡s reseÃ±ados, mejor valorados por promedio de puntuaciÃ³n y por sentimientos.

5. **VisualizaciÃ³n de Datos (`visualizacion.py`)**
   - Clase `DataVisualization`: Genera grÃ¡ficos utilizando Matplotlib y Seaborn para mostrar los libros mÃ¡s reseÃ±ados, autores mÃ¡s populares, distribuciÃ³n de sentimientos y mÃ¡s.

6. **Archivo Principal (`main.py`)**
   - Integra todas las funcionalidades de los mÃ³dulos anteriores y permite la ejecuciÃ³n completa del flujo de anÃ¡lisis.

---

Requisitos Previos

1. Crear un Ambiente Virtual

Se recomienda crear un entorno virtual para gestionar las dependencias. Siga los siguientes pasos:

python -m venv .venv
source .venv/bin/activate  # En Linux/MacOS
.venv\Scripts\activate    # En Windows

2. Instalar Dependencias

Las dependencias necesarias estÃ¡n especificadas en el archivo requirements.txt. Para instalarlas, ejecute:

pip install -r requirements.txt

3. UbicaciÃ³n de los Datos

Los archivos CSV descargados desde Kaggle ejecutando (`download_df.py`), mÃ³dulo encargado de descargar las bases de datos de Kaggle por primera vez en la ubicaciÃ³n:

data/raw/

El archivo de registros de transacciones, informaciÃ³n del consumidor y datos de cartas se llaman: `transactions_data.csv`, `users_data.csv` y `cards_data.csv`, respectivamente.
---

## EjecuciÃ³n del Proyecto

El archivo principal de ciencia de datos (actividad 2) `2_modelo_opt.py` consolida todas las funcionalidades. Para ejecutarlo:

```bash
python 2_modelo_opt.py
```

Este archivo realiza las siguientes acciones:

1. Limpieza de datos.
2. Carga y preparaciÃ³n de archivos csv.
3. IngenierÃ­a de caracterÃ­sticas.
4. Entrenamiento del modelo LightGBM.
5. MÃ©tricas RMSE, R^2 y RMSE Relativo.

---

## Ejemplo de Uso

### Datos de Uso
 El conjunto de datos en el repositorio en Kaggle [este enlace](https://www.kaggle.com/datasets/computingvictor/transactions-fraud-datasets/data).

### Salida del Proyecto
- Modelo predictivo generado a partir de las caracterÃ­sticas independientes:
    [
        "current_age",
        "retirement_age",
        "birth_year",
        "birth_month",
        "per_capita_income",
        "yearly_income",
        "total_debt",
        "credit_score",
        "num_credit_cards",
        "debt_to_income_ratio",
        "avg_credit_limit",
        "total_credit_limit",
        "total_cards_issued",
        "pct_chip_cards",
        "any_dark_web_card",
        "mcc",
    ]
- CaracterÃ­stica Dependiente: ["total_spent"]
- MÃ©tricas para identificar la calidad del modelo generado y proponer alternativas.

---

## Estructura del Repositorio

```
â”œâ”€â”€ .venv/ # Entorno virtual de Python
â”‚
â”œâ”€â”€ answers/ # Carpeta con respuestas y scripts SQL
â”‚ â”œâ”€â”€ 1a_queries.sql
â”‚ â”œâ”€â”€ 1b_queries.sql
â”‚ â”œâ”€â”€ 1c_queries.sql
â”‚ â”œâ”€â”€ 1d_optimization.md # Archivo .md con ideas o recomendaciones de mejora para el manejo de BigData con SQL
â”‚ â””â”€â”€ 2_modelo_opt.py # Script Python optimizado para entrenamiento
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Datos sin procesar
â”‚ â”‚ â”œâ”€â”€ cards_data.csv
â”‚ â”‚ â”œâ”€â”€ transactions_data.csv
â”‚ â”‚ â”œâ”€â”€ users_data.csv
â”‚ â”‚ â”œâ”€â”€ train_fraud_labels.json
â”‚ â”‚ â””â”€â”€ __MACOSX/ # Carpeta generada por macOS (puede ser ignorada)
â”‚
â”œâ”€â”€ 0_download_df.py # Script auxiliar para descargar de Kaggle
â”œâ”€â”€ new_data.zip # Archivo comprimido con nuevos datos
â”œâ”€â”€ requirements.txt # Dependencias del proyecto (pip install -r)
â”œâ”€â”€ README.md # DocumentaciÃ³n principal del repositorio
â”œâ”€â”€ report.md # Resultado y conclusiones del repositorio
```

---


El modelo actual no es bueno.
EstÃ¡ muy lejos de ser usable para predicciones confiables de total_spent.

ðŸ§© Â¿Por quÃ© puede estar fallando?
1. Variables no predictivas
Muchas de las variables (como birth_year, num_credit_cards, etc.) no capturan bien el patrÃ³n de gasto por cliente y MCC.

2. Ruido en los datos
Columnas como amount y credit_limit fueron limpiadas desde formatos corruptos, lo que puede haber introducido error.

3. Falta de features relevantes
Tal vez el comportamiento de gasto depende de mÃ¡s variables no disponibles:

Historial de transacciones (series de tiempo).

LocalizaciÃ³n del cliente.

Tipo de comercio asociado al MCC (algunas categorÃ­as gastan mÃ¡s por naturaleza).

Tiempo (estacionalidad: mes, dÃ­a de semana, etc.).